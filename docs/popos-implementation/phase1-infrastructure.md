# ğŸ“‹ Phase 1: Infrastructure Setup - ìƒì„¸ êµ¬í˜„ ê°€ì´ë“œ

> **Phase**: 1 of 6  
> **Duration**: Week 1  
> **Priority**: Critical  
> **Dependencies**: None

---

## ğŸ¯ ëª©í‘œ

PopOS ì„œë²„ í™˜ê²½ì„ ì™„ë²½í•˜ê²Œ êµ¬ì¶•í•˜ê³ , Flash Attentionê³¼ ë“€ì–¼ GPUë¥¼ ìœ„í•œ ê¸°ë°˜ì„ ë§ˆë ¨í•©ë‹ˆë‹¤.

---

## ğŸ“ ì‘ì—… ëª©ë¡

### 1. PopOS ì„œë²„ í™˜ê²½ ì„¤ì •

#### 1.1 ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
```bash
# PopOS ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
sudo apt update && sudo apt upgrade -y

# í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜
sudo apt install -y \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    htop \
    nvtop \
    screen \
    tmux
```

#### 1.2 Python í™˜ê²½ êµ¬ì¶•
```bash
# Python 3.10+ í™•ì¸
python3 --version

# pip ì—…ê·¸ë ˆì´ë“œ
pip3 install --upgrade pip

# ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv ~/wan_env
source ~/wan_env/bin/activate

# ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install wheel setuptools
```

---

### 2. NVIDIA ë“œë¼ì´ë²„ ë° CUDA ì„¤ì¹˜

#### 2.1 ë“œë¼ì´ë²„ ì„¤ì¹˜
```bash
# í˜„ì¬ ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# í•„ìš”ì‹œ ìµœì‹  ë“œë¼ì´ë²„ ì„¤ì¹˜
sudo apt install nvidia-driver-545

# ì¬ë¶€íŒ…
sudo reboot
```

#### 2.2 CUDA Toolkit ì„¤ì¹˜
```bash
# CUDA 12.3 ì„¤ì¹˜
wget https://developer.download.nvidia.com/compute/cuda/12.3.0/local_installers/cuda_12.3.0_545.23.06_linux.run
sudo sh cuda_12.3.0_545.23.06_linux.run

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo 'export PATH=/usr/local/cuda-12.3/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.3/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# í™•ì¸
nvcc --version
```

#### 2.3 cuDNN ì„¤ì¹˜
```bash
# cuDNN 8.9 ë‹¤ìš´ë¡œë“œ (NVIDIA ê³„ì • í•„ìš”)
# https://developer.nvidia.com/cudnn ì—ì„œ ë‹¤ìš´ë¡œë“œ

# ì„¤ì¹˜
tar -xzvf cudnn-linux-x86_64-8.9.*.tar.gz
sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda/include
sudo cp cudnn-*-archive/lib/libcudnn* /usr/local/cuda/lib64
sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*
```

---

### 3. PyTorch ë° Flash Attention ì„¤ì¹˜

#### 3.1 PyTorch ì„¤ì¹˜
```bash
# PyTorch 2.2.0 with CUDA 12.1
pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu121

# í™•ì¸
python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"
```

#### 3.2 Flash Attention ì„¤ì¹˜
```bash
# ì»´íŒŒì¼ ë„êµ¬ ì„¤ì¹˜
pip install ninja

# Flash Attention 2.5.0 ì„¤ì¹˜
pip install flash-attn==2.5.0

# ì„¤ì¹˜ ì‹¤íŒ¨ ì‹œ ì†ŒìŠ¤ì—ì„œ ë¹Œë“œ
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
pip install -e .

# í™•ì¸
python -c "from flash_attn import flash_attn_func; print('Flash Attention installed successfully')"
```

#### 3.3 xFormers ì„¤ì¹˜ (ë°±ì—…)
```bash
# xFormers ì„¤ì¹˜
pip install xformers==0.0.24

# í™•ì¸
python -c "import xformers; print(xformers.__version__)"
```

---

### 4. WAN ëª¨ë¸ í™˜ê²½ ì„¤ì •

#### 4.1 í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install \
    transformers==4.38.0 \
    diffusers==0.26.0 \
    accelerate==0.27.0 \
    safetensors \
    omegaconf \
    einops \
    timm \
    scipy \
    ftfy \
    imageio[ffmpeg] \
    opencv-python
```

#### 4.2 ëª¨ë¸ ê²½ë¡œ í™•ì¸
```bash
# ëª¨ë¸ ì¡´ì¬ í™•ì¸
ls -la ~/Wan2.2-*/

# ê° ëª¨ë¸ í¬ê¸° í™•ì¸
du -sh ~/Wan2.2-T2V-A14B/
du -sh ~/Wan2.2-I2V-A14B/
du -sh ~/Wan2.2-TI2V-5B/
du -sh ~/Wan2.2-S2V-14B/
```

#### 4.3 í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
```python
# test_environment.py
#!/usr/bin/env python3
import torch
import sys

print("=" * 50)
print("Environment Test")
print("=" * 50)

# PyTorch
print(f"PyTorch Version: {torch.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA Version: {torch.version.cuda}")
    print(f"GPU Count: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")

# Flash Attention
try:
    import flash_attn
    print(f"Flash Attention: âœ… Version {flash_attn.__version__}")
except ImportError:
    print("Flash Attention: âŒ Not installed")

# xFormers
try:
    import xformers
    print(f"xFormers: âœ… Version {xformers.__version__}")
except ImportError:
    print("xFormers: âŒ Not installed")

print("=" * 50)
```

---

### 5. ë„¤íŠ¸ì›Œí¬ ìµœì í™”

#### 5.1 MTU ì„¤ì •
```bash
# í˜„ì¬ MTU í™•ì¸
ip link show

# Jumbo Frames ì„¤ì • (9000 MTU)
sudo ip link set dev eth0 mtu 9000

# ì˜êµ¬ ì„¤ì • (/etc/network/interfaces)
# mtu 9000

# í™•ì¸
ping -M do -s 8972 10.0.0.1
```

#### 5.2 ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
```bash
# iperf3 ì„¤ì¹˜
sudo apt install iperf3

# ì„œë²„ ëª¨ë“œ ì‹¤í–‰ (PopOS)
iperf3 -s

# í´ë¼ì´ì–¸íŠ¸ì—ì„œ í…ŒìŠ¤íŠ¸ (Windows)
# iperf3 -c 10.0.0.2 -t 30
```

---

### 6. ì„œë¹„ìŠ¤ ì„¤ì •

#### 6.1 ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ ìƒì„±
```bash
# /etc/systemd/system/wan-server.service
sudo tee /etc/systemd/system/wan-server.service << EOF
[Unit]
Description=WAN Generation Server
After=network.target

[Service]
Type=simple
User=choon
WorkingDirectory=/home/choon
Environment="PATH=/home/choon/wan_env/bin:/usr/local/cuda/bin:/usr/bin"
ExecStart=/home/choon/wan_env/bin/python /home/choon/popos_wan_server_pro.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
EOF

# ì„œë¹„ìŠ¤ í™œì„±í™”
sudo systemctl daemon-reload
sudo systemctl enable wan-server
```

#### 6.2 GPU ì„±ëŠ¥ ëª¨ë“œ ì„¤ì •
```bash
# ìµœëŒ€ ì„±ëŠ¥ ëª¨ë“œ
sudo nvidia-smi -pm 1
sudo nvidia-smi -pl 350  # RTX 3090 ìµœëŒ€ ì „ë ¥

# ìë™ ì‹œì‘ ì„¤ì •
echo "nvidia-smi -pm 1" | sudo tee -a /etc/rc.local
echo "nvidia-smi -pl 350" | sudo tee -a /etc/rc.local
```

---

### 7. ê²€ì¦ í…ŒìŠ¤íŠ¸

#### 7.1 ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
```python
# test_connection.py
import requests

# API ì„œë²„ í…ŒìŠ¤íŠ¸
response = requests.get("http://10.0.0.2:8001/")
print(f"Server Status: {response.status_code}")

# Flash Attention í…ŒìŠ¤íŠ¸
response = requests.get("http://10.0.0.2:8001/flash/status")
print(f"Flash Attention: {response.json()}")
```

#### 7.2 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```python
# benchmark_flash.py
import torch
import time

def benchmark_attention(use_flash=False):
    batch_size = 4
    seq_len = 2048
    num_heads = 16
    head_dim = 64
    
    q = torch.randn(batch_size, num_heads, seq_len, head_dim).cuda().half()
    k = torch.randn(batch_size, num_heads, seq_len, head_dim).cuda().half()
    v = torch.randn(batch_size, num_heads, seq_len, head_dim).cuda().half()
    
    # Warmup
    for _ in range(10):
        if use_flash:
            from flash_attn import flash_attn_func
            _ = flash_attn_func(q, k, v)
        else:
            scores = torch.matmul(q, k.transpose(-2, -1)) / (head_dim ** 0.5)
            attn = torch.nn.functional.softmax(scores, dim=-1)
            _ = torch.matmul(attn, v)
    
    torch.cuda.synchronize()
    start = time.time()
    
    for _ in range(100):
        if use_flash:
            _ = flash_attn_func(q, k, v)
        else:
            scores = torch.matmul(q, k.transpose(-2, -1)) / (head_dim ** 0.5)
            attn = torch.nn.functional.softmax(scores, dim=-1)
            _ = torch.matmul(attn, v)
    
    torch.cuda.synchronize()
    elapsed = time.time() - start
    
    return elapsed

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
standard_time = benchmark_attention(False)
flash_time = benchmark_attention(True)

print(f"Standard Attention: {standard_time:.3f}s")
print(f"Flash Attention: {flash_time:.3f}s")
print(f"Speedup: {standard_time/flash_time:.2f}x")
```

---

## âœ… ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] PopOS ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸ ì™„ë£Œ
- [ ] Python 3.10+ í™˜ê²½ êµ¬ì¶•
- [ ] NVIDIA Driver 545+ ì„¤ì¹˜
- [ ] CUDA 12.3+ ì„¤ì¹˜
- [ ] cuDNN 8.9+ ì„¤ì¹˜
- [ ] PyTorch 2.2.0 ì„¤ì¹˜ ë° CUDA í™•ì¸
- [ ] Flash Attention 2.5.0 ì„¤ì¹˜ ë° í…ŒìŠ¤íŠ¸
- [ ] xFormers ì„¤ì¹˜ (ë°±ì—…)
- [ ] WAN ëª¨ë¸ ê²½ë¡œ í™•ì¸
- [ ] ë„¤íŠ¸ì›Œí¬ MTU 9000 ì„¤ì •
- [ ] 10Gbps ì†ë„ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] GPU ì„±ëŠ¥ ëª¨ë“œ ì„¤ì •
- [ ] ì„œë¹„ìŠ¤ ìë™ ì‹œì‘ ì„¤ì •
- [ ] ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼

---

## ğŸš¨ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### Flash Attention ì„¤ì¹˜ ì‹¤íŒ¨
```bash
# GCC ë²„ì „ í™•ì¸ (11+ í•„ìš”)
gcc --version

# í•„ìš”ì‹œ GCC 11 ì„¤ì¹˜
sudo apt install gcc-11 g++-11
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 100
```

### CUDA ë©”ëª¨ë¦¬ ì˜¤ë¥˜
```bash
# ìºì‹œ ì •ë¦¬
torch.cuda.empty_cache()

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

### ë„¤íŠ¸ì›Œí¬ ì§€ì—°
```bash
# TCP ìµœì í™”
sudo sysctl -w net.core.rmem_max=134217728
sudo sysctl -w net.core.wmem_max=134217728
sudo sysctl -w net.ipv4.tcp_rmem="4096 87380 134217728"
sudo sysctl -w net.ipv4.tcp_wmem="4096 65536 134217728"
```

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- [Flash Attention GitHub](https://github.com/Dao-AILab/flash-attention)
- [PyTorch CUDA Setup](https://pytorch.org/get-started/locally/)
- [NVIDIA Driver Installation](https://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/)

---

## â­ï¸ ë‹¤ìŒ ë‹¨ê³„: Phase 2

Phase 1ì´ ì™„ë£Œë˜ë©´ Phase 2 (Core Server Implementation)ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.

---

*ì´ ë¬¸ì„œëŠ” Phase 1ì˜ ëª¨ë“  ì‘ì—…ì„ ìƒì„¸íˆ ì•ˆë‚´í•©ë‹ˆë‹¤.*  
*ë¬¸ì œ ë°œìƒ ì‹œ íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ì„¹ì…˜ì„ ì°¸ì¡°í•˜ì„¸ìš”.*